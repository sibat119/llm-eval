{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fields=[\"high_school_computer_science\", \"philosophy\", \"public_relations\"]\n",
    "prompt_strategies=[\"black_box\"]\n",
    "selection_strategies=[\"random\"]\n",
    "\n",
    "# Base models\n",
    "llama=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "qwen=\"Qwen/Qwen2.5-7B-Instruct\"\n",
    "batch_size=16\n",
    "shot=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 6 examples [00:00, 726.06 examples/s]\n",
      "Generating train split: 15 examples [00:00, 1874.07 examples/s]\n",
      "Generating train split: 20 examples [00:00, 2562.66 examples/s]\n",
      "Generating train split: 38 examples [00:00, 3863.28 examples/s]\n",
      "Generating train split: 6 examples [00:00, 989.30 examples/s]\n",
      "Generating train split: 12 examples [00:00, 1903.04 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Dataset({\n",
       "      features: ['zero_shot_response', 'few_shot_response', 'black_box_response', 'prompt', 'zero_shot_top_index', 'few_shot_top_index', 'prompt_pick_cosine_sim_zero', 'prompt_pick_cosine_sim_few'],\n",
       "      num_rows: 6\n",
       "  }),\n",
       "  '/home/sibat/repoes/llm-eval/data/dataset/surrogate/high_school_computer_science/black_box/5-shot-random-selection/agreement-to-disagreement-candidate-meta-llama-Llama-3.1-8B-Instruct-surrogate-Qwen-Qwen2.5-7B-Instruct.csv'),\n",
       " (Dataset({\n",
       "      features: ['zero_shot_response', 'few_shot_response', 'black_box_response', 'prompt', 'zero_shot_top_index', 'few_shot_top_index', 'prompt_pick_cosine_sim_zero', 'prompt_pick_cosine_sim_few'],\n",
       "      num_rows: 15\n",
       "  }),\n",
       "  '/home/sibat/repoes/llm-eval/data/dataset/surrogate/high_school_computer_science/black_box/5-shot-random-selection/agreement-to-disagreement-candidate-Qwen-Qwen2.5-7B-Instruct-surrogate-meta-llama-Llama-3.1-8B-Instruct.csv'),\n",
       " (Dataset({\n",
       "      features: ['zero_shot_response', 'few_shot_response', 'black_box_response', 'prompt', 'zero_shot_top_index', 'few_shot_top_index', 'prompt_pick_cosine_sim_zero', 'prompt_pick_cosine_sim_few'],\n",
       "      num_rows: 20\n",
       "  }),\n",
       "  '/home/sibat/repoes/llm-eval/data/dataset/surrogate/philosophy/black_box/5-shot-random-selection/agreement-to-disagreement-candidate-meta-llama-Llama-3.1-8B-Instruct-surrogate-Qwen-Qwen2.5-7B-Instruct.csv'),\n",
       " (Dataset({\n",
       "      features: ['zero_shot_response', 'few_shot_response', 'black_box_response', 'prompt', 'zero_shot_top_index', 'few_shot_top_index', 'prompt_pick_cosine_sim_zero', 'prompt_pick_cosine_sim_few'],\n",
       "      num_rows: 38\n",
       "  }),\n",
       "  '/home/sibat/repoes/llm-eval/data/dataset/surrogate/philosophy/black_box/5-shot-random-selection/agreement-to-disagreement-candidate-Qwen-Qwen2.5-7B-Instruct-surrogate-meta-llama-Llama-3.1-8B-Instruct.csv'),\n",
       " (Dataset({\n",
       "      features: ['zero_shot_response', 'few_shot_response', 'black_box_response', 'prompt', 'zero_shot_top_index', 'few_shot_top_index', 'prompt_pick_cosine_sim_zero', 'prompt_pick_cosine_sim_few'],\n",
       "      num_rows: 6\n",
       "  }),\n",
       "  '/home/sibat/repoes/llm-eval/data/dataset/surrogate/public_relations/black_box/5-shot-random-selection/agreement-to-disagreement-candidate-meta-llama-Llama-3.1-8B-Instruct-surrogate-Qwen-Qwen2.5-7B-Instruct.csv'),\n",
       " (Dataset({\n",
       "      features: ['zero_shot_response', 'few_shot_response', 'black_box_response', 'prompt', 'zero_shot_top_index', 'few_shot_top_index', 'prompt_pick_cosine_sim_zero', 'prompt_pick_cosine_sim_few'],\n",
       "      num_rows: 12\n",
       "  }),\n",
       "  '/home/sibat/repoes/llm-eval/data/dataset/surrogate/public_relations/black_box/5-shot-random-selection/agreement-to-disagreement-candidate-Qwen-Qwen2.5-7B-Instruct-surrogate-meta-llama-Llama-3.1-8B-Instruct.csv')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets = []\n",
    "for sub_field in sub_fields:\n",
    "    for selection_strategy in selection_strategies:\n",
    "        for prompt_variation in prompt_strategies:\n",
    "            surrogate_dir = os.path.join('/home/sibat/repoes/llm-eval/data/dataset', 'surrogate', sub_field, prompt_variation, f\"{shot}-shot-{selection_strategy}-selection\")\n",
    "            candidate_llm = llama\n",
    "            surrogate_llm = qwen\n",
    "            ds_path = f\"{surrogate_dir}/agreement-to-disagreement-candidate-{candidate_llm.replace('/', '-')}-surrogate-{surrogate_llm.replace('/', '-')}.csv\"\n",
    "            ds = Dataset.from_csv(ds_path)\n",
    "            all_datasets.append((ds, ds_path))\n",
    "            \n",
    "            candidate_llm = qwen\n",
    "            surrogate_llm = llama\n",
    "            ds_path = f\"{surrogate_dir}/agreement-to-disagreement-candidate-{candidate_llm.replace('/', '-')}-surrogate-{surrogate_llm.replace('/', '-')}.csv\"\n",
    "            ds = Dataset.from_csv(ds_path)\n",
    "            all_datasets.append((ds, ds_path))\n",
    "\n",
    "all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-candidate-meta-llama-Llama-3.1-8B-Instruct-surrogate-Qwen-Qwen2.5-7B-Instruct.csv']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index +=1\n",
    "ds, dspath = all_datasets[index]\n",
    "i = -1\n",
    "dspath.split('agreement-to-disagreement')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "zero_shot_response\n",
      "implausibly places an \"upper bound\" on moral worthiness.\n",
      "few_shot_response\n",
      "yields an unattractive picture of moral sainthood.\n",
      "black_box_response\n",
      "implausibly places an “upper bound” on moral worthiness.\n",
      "zero_shot_top_index\n",
      "1\n",
      "few_shot_top_index\n",
      "0\n",
      "prompt_pick_cosine_sim_zero\n",
      "0.0817261412739753\n",
      "prompt_pick_cosine_sim_few\n",
      "0.0337408408522605\n",
      "prompt\n",
      "Your goal is to predict what the black-box model will answer to a given question. Below are examples of inputs (questions) and the black-box model's actual responses.\n",
      "\n",
      "The black-box LLM saw the following instructions each time it answered:\n",
      "1. System instruction: \"You are a helpful AI assistant answering questions. Provide only accurate answers in natural language.\"\n",
      "2. Prompt structure given to black-box LLM:\n",
      "  \"\"\"\n",
      "  Given the following question and candidate answers, choose the best answer. \n",
      "  Do not include option labels (A, B, C, D); respond only with natural answer text. \n",
      "  Provide only the answer text in your response.\n",
      "\n",
      "  Question: <question_text>\n",
      "  Options: <option_texts>\n",
      "  \"\"\"\n",
      "\n",
      "Examples of the Black-Box LLM in Action:\n",
      "Example 1:\n",
      "Question: \"Moore defines the naturalistic fallacy as the view that:\"\n",
      "Options: \"- everything that is natural is good.\n",
      "- everything that is good is natural.\n",
      "- good is identical to a natural property.\n",
      "- all of the above.\"\n",
      "Response: \"everything that is good is natural.\"\n",
      "\n",
      "Example 2:\n",
      "Question: \"Berkeley believes that sensible things cannot exist except in _____.\"\n",
      "Options: \"- a mind\n",
      "- absolute existence\n",
      "- material substance\n",
      "- a material universe\"\n",
      "Response: \"<|start_header_id|>assistant\n",
      "\n",
      "a mind\"\n",
      "\n",
      "Example 3:\n",
      "Question: \"According to d'Holbach, all the mental and moral attributes that people think are evidence for an immaterial soul are in fact _____.\"\n",
      "Options: \"- purely intellectual\n",
      "- purely physical and natural\n",
      "- ethereal\n",
      "- undetermined\"\n",
      "Response: \"<|start_header_id|>assistant\n",
      "\n",
      "purely physical and natural.\"\n",
      "\n",
      "Example 4:\n",
      "Question: \"Which of the following is a form of harm that might be suffered by research participants?\"\n",
      "Options: \"- Physical injury\n",
      "- Stress and anxiety\n",
      "- Impaired development\n",
      "- All of the above\"\n",
      "Response: \"All of the above.\"\n",
      "\n",
      "Example 5:\n",
      "Question: \"According to Kant, moral laws are:\"\n",
      "Options: \"- necessary and apply to all rational beings.\n",
      "- contingent and apply only to human beings.\n",
      "- culturally relative.\n",
      "- grounded in God’s commands.\"\n",
      "Response: \"<|start_header_id|>assistant\n",
      "\n",
      "necessary and apply to all rational beings.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Based solely on these examples, predict the most likely response from the black-box model to this new question. \n",
      "New Question: Wolf claims that interpreting Kant’s ethics so that it consists of a finite set of constraints:\n",
      "Options: - yields an unattractive picture of moral sainthood.\n",
      "- implausibly places an “upper bound” on moral worthiness.\n",
      "- yields a theory that is too demanding.\n",
      "- transforms the theory into a form of contractarianism.\n",
      "Your Predicted Black-box LLM Response: (Exact response text only)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i += 1\n",
    "print(i)\n",
    "\n",
    "print(f\"zero_shot_response\")\n",
    "print(f\"{ds['zero_shot_response'][i]}\")\n",
    "print(f\"few_shot_response\")\n",
    "print(f\"{ds['few_shot_response'][i]}\")\n",
    "print(f\"black_box_response\")\n",
    "print(f\"{ds['black_box_response'][i]}\")\n",
    "\n",
    "print(\"zero_shot_top_index\")\n",
    "print(f\"{ds['zero_shot_top_index'][i]}\")\n",
    "print(\"few_shot_top_index\")\n",
    "print(f\"{ds['few_shot_top_index'][i]}\")\n",
    "print(\"prompt_pick_cosine_sim_zero\")\n",
    "print(f\"{ds['prompt_pick_cosine_sim_zero'][i]}\")\n",
    "print(\"prompt_pick_cosine_sim_few\")\n",
    "print(f\"{ds['prompt_pick_cosine_sim_few'][i]}\")\n",
    "print(f\"prompt\")\n",
    "print(f\"{ds['prompt'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 3, 0, 2, 0, 2, 2, 3, 1, 0, 0, 0, 3, 3]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['few_shot_top_index']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
